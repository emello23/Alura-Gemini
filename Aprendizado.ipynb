{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emello23/Alura-Gemini/blob/main/Aprendizado.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\"\"\"\n",
        "Script para extrair texto de um PDF, gerar resumo e tópicos com a API do Gemini\n",
        "e criar um chatbot para interagir com o conteúdo.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ENtx9IF7x7jX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KvjdxxJooi_y"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9AprP9CDqZpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Montar o Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "s5dYG2kDdbZ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f0e3ffd-63a0-4a4c-9d25-4f3ba472a24e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar/Atualizar as bibliotecas necessárias\n",
        "!pip install --upgrade google-genai PyMuPDF\n"
      ],
      "metadata": {
        "id": "lGCELBEr-InA",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97809561-0747-4b3b-9e6f-273079703b99"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-genai in /usr/local/lib/python3.11/dist-packages (1.15.0)\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.11/dist-packages (1.25.5)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (4.9.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.38.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.11.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.32.3)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (15.0.1)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (1.3.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (4.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.4.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from google import genai\n",
        "from google.genai import types # Importação ativa\n",
        "import fitz  # Importa a biblioteca PyMuPDF"
      ],
      "metadata": {
        "id": "Du_leth3cD3v"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importa classes do módulo IPython.display para formatar a saída no notebook.\n",
        "# - HTML: Permite exibir conteúdo HTML diretamente.\n",
        "# - Markdown: Permite exibir conteúdo formatado em Markdown.\n",
        "\n",
        "from IPython.display import HTML, Markdown"
      ],
      "metadata": {
        "id": "8Cq1z9zkk14A"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurar a chave da API\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')\n",
        "client = genai.Client()"
      ],
      "metadata": {
        "id": "vqkXAdE8cJxT"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir o modelo a ser usado\n",
        "modelo = \"gemini-2.0-flash\"\n"
      ],
      "metadata": {
        "id": "gL-FVrZVcX38"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PARTE ONDE VOCÊ CARREGA O PDF ---\n",
        "# Atualize o caminho do PDF para apontar para o seu arquivo no Google Drive\n",
        "# Alternativa: Coloque o arquivo na sua pasta raiz do My Drive\n",
        "#              e renomei-o para aprendizado.pdf mantendo este mesmo caminho.\n",
        "caminho_do_pdf = \"/content/drive/My Drive/aprendizado.pdf\" # <-- ATUALIZE ESTA LINHA COM O CAMINHO CORRETO'"
      ],
      "metadata": {
        "id": "4WNFIXhQl0Ff"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extrair_texto_de_pdf(caminho_do_arquivo):\n",
        "    \"\"\"Extrai texto de um arquivo PDF.\"\"\"\n",
        "    texto_completo = \"\"\n",
        "    try:\n",
        "        documento = fitz.open(caminho_do_arquivo)\n",
        "        for pagina_num in range(documento.page_count):\n",
        "            pagina = documento.load_page(pagina_num)\n",
        "            texto_completo += pagina.get_text()\n",
        "        documento.close()\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao extrair texto do PDF: {e}\")\n",
        "        texto_completo = None # Retorna None em caso de erro\n",
        "    return texto_completo\n",
        "\n"
      ],
      "metadata": {
        "id": "SVIA1oYHcj1g"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texto_do_pdf = extrair_texto_de_pdf(caminho_do_pdf)\n",
        "\n"
      ],
      "metadata": {
        "id": "aRPFYuJscmC2",
        "collapsed": true
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " if texto_do_pdf: # Verifica se a extração do texto foi bem-sucedida\n",
        "    display(Markdown(\"### Texto extraído do PDF (primeiros 500 caracteres):\"))\n",
        "    display(Markdown(texto_do_pdf[:500] + \"...\")) # Mostra apenas uma parte do texto para não sobrecarregar\n",
        "\n",
        "    # --- USANDO GEMINI PARA AJUDAR COM RESUMO E TÓPICOS ---\n",
        "    # Gerar um resumo inicial\n",
        "    try:\n",
        "        response_resumo = client.models.generate_content(\n",
        "            model=modelo,\n",
        "            contents=f\"Por favor, gere um resumo conciso do seguinte texto:\\n\\n{texto_do_pdf}\"\n",
        "        )\n",
        "        if response_resumo and response_resumo.text: # Verifica se a resposta e o texto existem\n",
        "            resumo = response_resumo.text\n",
        "            display(Markdown(\"\\n### Resumo gerado pelo Gemini:\"))\n",
        "            display(Markdown(resumo))\n",
        "        else:\n",
        "            display(Markdown(\"\\n**Não foi possível gerar um resumo.**\"))\n",
        "    except Exception as e:\n",
        "        display(Markdown(f\"**Erro ao gerar resumo:** {e}\"))\n",
        "\n",
        "\n",
        "    # Tentar identificar tópicos principais\n",
        "    try:\n",
        "        response_topicos = client.models.generate_content(\n",
        "            model=modelo,\n",
        "            contents=f\"Por favor, liste os principais tópicos e sub-tópicos (numerados se possível) encontrados no seguinte texto:\\n\\n{texto_do_pdf}\"\n",
        "        )\n",
        "        if response_topicos and response_topicos.text: # Verifica se a resposta e o texto existem\n",
        "            topicos_identificados = response_topicos.text\n",
        "            display(Markdown(\"\\n### Tópicos identificados pelo Gemini:\"))\n",
        "            display(Markdown(topicos_identificados))\n",
        "        else:\n",
        "            display(Markdown(\"\\n**Não foi possível identificar tópicos.**\"))\n",
        "    except Exception as e:\n",
        "        display(Markdown(f\"**Erro ao identificar tópicos:** {e}\"))\n",
        "\n",
        "    # --- PARTE DO CHATBOT ---\n",
        "    display(Markdown(\"\\n### Chatbot - Aprenda sobre o conteúdo do documento\"))\n",
        "    display(Markdown(\"Ok! Vamos aprender sobre o conteúdo do documento.\"))\n",
        "    display(Markdown(\"Eu farei perguntas sobre o conteúdo do documento. Tente respondê-las!\"))\n",
        "    display(Markdown(\"Digite 'fim' para encerrar o chat.\"))\n",
        "    display(Markdown(\"Desconsidere a 'Pergunta 1:' digitando 'OK'\"))\n",
        "    display(Markdown(\"\\n**Vamos começar!**\"))\n",
        "\n",
        "    # Abordagem para gerar perguntas\n",
        "    try:\n",
        "        response_perguntas = client.models.generate_content(\n",
        "            model=modelo,\n",
        "            contents=f\"Com base no seguinte texto, gere 10 perguntas sobre o seu conteúdo para que um usuário possa respondê-las. Liste as perguntas numeradas.\\n\\n{texto_do_pdf}\"\n",
        "        )\n",
        "\n",
        "        if response_perguntas and response_perguntas.text: # Verifica se a resposta e o texto existem\n",
        "            perguntas_geradas = response_perguntas.text.split('\\n') # Divide as perguntas em uma lista\n",
        "            # Filtra linhas vazias e numerações que não são perguntas válidas\n",
        "            perguntas_validas = [p.strip() for p in perguntas_geradas if p.strip() and any(char.isdigit() for char in p.strip().split('.')[0])]\n",
        "\n",
        "\n",
        "            if not perguntas_validas:\n",
        "                display(Markdown(\"**Não foi possível gerar perguntas a partir do PDF.**\"))\n",
        "            else:\n",
        "                indice_pergunta_atual = 0\n",
        "\n",
        "                while indice_pergunta_atual < len(perguntas_validas):\n",
        "                    pergunta_atual = perguntas_validas[indice_pergunta_atual]\n",
        "                    display(Markdown(f\"\\n**Pergunta {indice_pergunta_atual + 1}:** {pergunta_atual}\"))\n",
        "\n",
        "                    resposta_usuario = input(\"Sua resposta: \")\n",
        "\n",
        "                    if resposta_usuario.lower() == \"fim\":\n",
        "                        display(Markdown(\"**Chat encerrado. Até a próxima!**\"))\n",
        "                        break\n",
        "\n",
        "                    # Usar o Gemini para avaliar a resposta do usuário\n",
        "                    try:\n",
        "                        chat_avaliacao_config = types.GenerateContentConfig(\n",
        "                          system_instruction = f\"Você é um avaliador de respostas baseado no seguinte texto. A pergunta foi: '{pergunta_atual}'. Avalie a resposta do usuário ('{resposta_usuario}') com base no texto original e diga se ela está correta, incorreta ou parcialmente correta, explicando brevemente por que. O texto original é:\\n\\n{texto_do_pdf}\"\n",
        "                        )\n",
        "                        chat_avaliacao = client.chats.create(model=modelo, config=chat_avaliacao_config)\n",
        "                        response_avaliacao = chat_avaliacao.send_message(f\"Pergunta: {pergunta_atual}\\nResposta do usuário: {resposta_usuario}\")\n",
        "\n",
        "                        if response_avaliacao and response_avaliacao.text: # Verifica se a resposta e o texto existem\n",
        "                            display(Markdown(f\"**Avaliação do Gemini:** {response_avaliacao.text}\"))\n",
        "                        else:\n",
        "                            display(Markdown(\"**Não foi possível avaliar a resposta.**\"))\n",
        "\n",
        "                    except Exception as e:\n",
        "                        display(Markdown(f\"**Erro ao avaliar a resposta:** {e}\"))\n",
        "\n",
        "                    indice_pergunta_atual += 1 # Passa para a próxima pergunta\n",
        "\n",
        "                if indice_pergunta_atual == len(perguntas_validas):\n",
        "                  display(Markdown(\"\\n**Você respondeu a todas as perguntas. Chat encerrado. Até a próxima!**\"))\n",
        "        else:\n",
        "            display(Markdown(\"**Não foi possível gerar perguntas a partir do PDF.**\"))\n",
        "    except Exception as e:\n",
        "        display(Markdown(f\"**Erro ao gerar perguntas ou executar o loop do chatbot:** {e}\"))\n",
        "\n",
        "else:\n",
        "    display(Markdown(\"**Não foi possível prosseguir porque a extração do texto do PDF falhou.**\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "iO2673OTsisK",
        "outputId": "235e294a-a935-4c83-84cd-0c6817f43998"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Inglaterra' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-54438685d38c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mInglaterra\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Inglaterra' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if texto_do_pdf: # Verifica se a extração do texto foi bem-sucedida\n",
        "    # --- Alteração aqui para imprimir em Markdown ---\n",
        "    display(Markdown(\"## Texto extraído do PDF (primeiros 500 caracteres):\"))\n",
        "    print(texto_do_pdf[:500] + \"...\") # Este print pode permanecer, pois é um trecho longo\n",
        "    # --- Fim da alteração ---\n",
        "\n",
        "    # --- USANDO GEMINI PARA AJUDAR COM RESUMO E TÓPICOS ---\n",
        "    # Gerar um resumo inicial\n",
        "    try:\n",
        "        response_resumo = client.models.generate_content(\n",
        "            model=modelo,\n",
        "            contents=f\"Por favor, gere um resumo conciso do seguinte texto:\\n\\n{texto_do_pdf}\"\n",
        "        )\n",
        "        if response_resumo and response_resumo.text: # Verifica se a resposta e o texto existem\n",
        "            resumo = response_resumo.text\n",
        "            # --- Alteração aqui para imprimir em Markdown ---\n",
        "            display(Markdown(\"## Resumo gerado pelo Gemini:\"))\n",
        "            display(Markdown(resumo))\n",
        "            # --- Fim da alteração ---\n",
        "        else:\n",
        "            # --- Alteração aqui para imprimir em Markdown ---\n",
        "            display(Markdown(\"## Não foi possível gerar um resumo.\"))\n",
        "            # --- Fim da alteração ---\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao gerar resumo: {e}\")\n",
        "\n",
        "\n",
        "    # Tentar identificar tópicos principais\n",
        "    try:\n",
        "        response_topicos = client.models.generate_content(\n",
        "            model=modelo,\n",
        "            contents=f\"Por favor, liste os principais tópicos e sub-tópicos (numerados se possível) encontrados no seguinte texto:\\n\\n{texto_do_pdf}\"\n",
        "        )\n",
        "        if response_topicos and response_topicos.text: # Verifica se a resposta e o texto existem\n",
        "            topicos_identificados = response_topicos.text\n",
        "            # --- Alteração aqui para imprimir em Markdown ---\n",
        "            display(Markdown(\"## Tópicos identificados pelo Gemini:\"))\n",
        "            display(Markdown(topicos_identificados))\n",
        "            # --- Fim da alteração ---\n",
        "        else:\n",
        "             # --- Alteração aqui para imprimir em Markdown ---\n",
        "            display(Markdown(\"## Não foi possível identificar tópicos.\"))\n",
        "            # --- Fim da alteração ---\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao identificar tópicos: {e}\")\n",
        "\n",
        "    # --- PARTE DO CHATBOT ---\n",
        "    # --- Alterações aqui para imprimir em Markdown ---\n",
        "    display(Markdown(\"## Ok! Vamos aprender sobre o conteúdo do documento.\"))\n",
        "    display(Markdown(\"Eu farei perguntas sobre o conteúdo do documento. Tente respondê-las!\"))\n",
        "    display(Markdown(\"Digite 'fim' para encerrar o chat.\"))\n",
        "    display(Markdown(\"Desconsidere a 'Pergunta 1:' digitando 'OK'\"))\n",
        "    display(Markdown(\"### Vamos começar!\"))\n",
        "    # --- Fim das alterações ---\n",
        "\n",
        "    # Abordagem para gerar perguntas\n",
        "    try:\n",
        "        response_perguntas = client.models.generate_content(\n",
        "            model=modelo,\n",
        "            contents=f\"Com base no seguinte texto, gere 10 perguntas sobre o seu conteúdo para que um usuário possa respondê-las. Liste as perguntas numeradas.\\n\\n{texto_do_pdf}\"\n",
        "        )\n",
        "\n",
        "        if response_perguntas and response_perguntas.text: # Verifica se a resposta e o texto existem\n",
        "            perguntas_geradas = response_perguntas.text.split('\\n') # Divide as perguntas em uma lista\n",
        "            # Filtra linhas vazias e numerações que não são perguntas válidas\n",
        "            perguntas_validas = [p.strip() for p in perguntas_geradas if p.strip() and any(char.isdigit() for char in p.strip().split('.')[0])]\n",
        "\n",
        "\n",
        "            if not perguntas_validas:\n",
        "                 # --- Alteração aqui para imprimir em Markdown ---\n",
        "                display(Markdown(\"## Não foi possível gerar perguntas a partir do PDF.\"))\n",
        "                 # --- Fim da alteração ---\n",
        "            else:\n",
        "                indice_pergunta_atual = 0\n",
        "\n",
        "                while indice_pergunta_atual < len(perguntas_validas):\n",
        "                    pergunta_atual = perguntas_validas[indice_pergunta_atual]\n",
        "                     # --- Alteração aqui para imprimir em Markdown ---\n",
        "                    display(Markdown(f\"### Pergunta {indice_pergunta_atual + 1}: {pergunta_atual}\"))\n",
        "                    # --- Fim da alteração ---\n",
        "\n",
        "                    resposta_usuario = input(\"Sua resposta: \")\n",
        "\n",
        "                    if resposta_usuario.lower() == \"fim\":\n",
        "                        print(\"Chat encerrado. Até a próxima!\") # Este print pode permanecer\n",
        "                        break\n",
        "\n",
        "                    # Usar o Gemini para avaliar a resposta do usuário\n",
        "                    try:\n",
        "                        chat_avaliacao_config = types.GenerateContentConfig(\n",
        "                          system_instruction = f\"Você é um avaliador de respostas baseado no seguinte texto. A pergunta foi: '{pergunta_atual}'. Avalie a resposta do usuário ('{resposta_usuario}') com base no texto original e diga se ela está correta, incorreta ou parcialmente correta, explicando brevemente por que. O texto original é:\\n\\n{texto_do_pdf}\"\n",
        "                        )\n",
        "                        chat_avaliacao = client.chats.create(model=modelo, config=chat_avaliacao_config)\n",
        "                        response_avaliacao = chat_avaliacao.send_message(f\"Pergunta: {pergunta_atual}\\nResposta do usuário: {resposta_usuario}\")\n",
        "\n",
        "                        if response_avaliacao and response_avaliacao.text: # Verifica se a resposta e o texto existem\n",
        "                             # --- Alteração aqui para imprimir em Markdown ---\n",
        "                            display(Markdown(\"### Avaliação do Gemini: \"))\n",
        "                            display(Markdown(response_avaliacao.text))\n",
        "                             # --- Fim da alteração ---\n",
        "                        else:\n",
        "                             # --- Alteração aqui para imprimir em Markdown ---\n",
        "                            display(Markdown(\"## Não foi possível avaliar a resposta.\"))\n",
        "                            # --- Fim da alteração ---\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Erro ao avaliar a resposta: {e}\")\n",
        "\n",
        "                    indice_pergunta_atual += 1 # Passa para a próxima pergunta\n",
        "\n",
        "                if indice_pergunta_atual == len(perguntas_validas):\n",
        "                   # --- Alteração aqui para imprimir em Markdown ---\n",
        "                   display(Markdown(\"\\n## Você respondeu a todas as perguntas. Chat encerrado. Até a próxima!\"))\n",
        "                   # --- Fim da alteração ---\n",
        "        else:\n",
        "             # --- Alteração aqui para imprimir em Markdown ---\n",
        "            display(Markdown(\"## Não foi possível gerar perguntas a partir do PDF.\"))\n",
        "            # --- Fim da alteração ---\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao gerar perguntas ou executar o loop do chatbot: {e}\")\n",
        "\n",
        "else:\n",
        "     # --- Alteração aqui para imprimir em Markdown ---\n",
        "    display(Markdown(\"## Não foi possível prosseguir porque a extração do texto do PDF falhou.\"))\n",
        "    # --- Fim da alteração ---"
      ],
      "metadata": {
        "id": "SfZMyLJtnNvh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}